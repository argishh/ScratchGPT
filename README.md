ScratchGPT is a GPT model built from scratch using theories based on paper titled **'Attention is all you need'**. This paper introduced the Transformer architecture.

The dataset used here is Tiny Shakespeare dataset, which contains all of Shakespeare's plays combined into a single file.

Concepts implemented - Transformer Architecture, Self-attention mechanism (decoder-attention), multi-headed self attention, residual connections, layernorm.

<br>
<be>

Also explore,

Google Colab notebook explaining the used approach - \
[![Google Colab Notebook](https://img.shields.io/badge/-Google_Colab_Notebook-05122A?style=flat&logo=googlecolab)](https://colab.research.google.com/drive/15UVgF8EPNkzIt85f1o6W81rCk5JtQ-yJ?usp=sharing)

YouTube tutorial -
[Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY) 

<br>

----

p.s. Thank You for an amazing tutorial [@AndrejKarpathy](https://github.com/karpathy)
